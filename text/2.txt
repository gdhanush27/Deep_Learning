from pyspark.sql import SparkSession
from pyspark.sql.functions import count
spark=SparkSession.builder.master("local[1]").appName("Videogame_window").getOrCreate()
df=spark.read.csv("/content/vgsales.csv",header=True,inferSchema=True)

column_name="Publisher"
df.groupBy(column_name).agg(count("*").alias("Count")).show()

from pyspark.sql.window import Window
from pyspark.sql.functions import row_number
windowspec=Window.partitionBy("Publisher").orderBy("Global_Sales")
df.withColumn("row_number",row_number().over(windowspec)).show(truncate=False)

from pyspark.sql.functions import rank
df.withColumn("rank",rank().over(windowspec)).show(5)

from pyspark.sql.functions import dense_rank
df.withColumn("dense_rank",dense_rank().over(windowspec)).show(5)

from pyspark.sql.functions import percent_rank
df.withColumn("percent_rank",percent_rank().over(windowspec)).show(5)

from pyspark.sql.functions import ntile
df.withColumn("ntile",ntile(2).over(windowspec)).show(5)

from pyspark.sql.functions import cume_dist
df.withColumn("cume_dist",cume_dist().over(windowspec)).show(5)

from pyspark.sql.functions import lag
df.withColumn("lag",lag("Global_Sales").over(windowspec)).show(5)

from pyspark.sql.functions import lead
df.withColumn("lead",lead("Global_Sales",2).over(windowspec)).show(5)

from pyspark.sql.window import Window
from pyspark.sql.functions import col,avg,sum,min,max,row_number
windowspecagg = Window.partitionBy("Publisher")
windowspec=Window.partitionBy("Publisher").orderBy("Global_Sales")
df.withColumn("row",row_number().over(windowspec))\
  .withColumn("avg",avg(col("Global_Sales")).over(windowspecagg))\
  .withColumn("sum",sum(col("Global_Sales")).over(windowspecagg))\
  .withColumn("min",min(col("Global_Sales")).over(windowspecagg))\
  .withColumn("max",max(col("Global_Sales")).over(windowspecagg))\
  .where(col("row")==1).select("Publisher","avg","sum","min","max").show()