from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import BinaryClassificationEvaluator

spark = SparkSession.builder.appName("HousePricePrediction").getOrCreate()
data = spark.read.csv("houseprice.csv", header=True, inferSchema=True)
for col in data.columns:
    data = data.withColumn(col, data[col].cast("double"))
feature_columns = [
    "LotArea",
    "OverallQual",
    "OverallCond",
    "TotalBsmtSF",
    "FullBath",
    "HalfBath",
    "BedroomAbvGr",
    "TotRmsAbvGrd",
    "Fireplaces",
    "GarageArea"
]
train_data, test_data = data.randomSplit([0.8, 0.2], seed=123)

assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
rf = RandomForestClassifier(labelCol="AboveMedianPrice", featuresCol="features")
pipeline = Pipeline(stages=[assembler, rf])
model = pipeline.fit(train_data)
predictions = model.transform(test_data)
evaluator = BinaryClassificationEvaluator(labelCol="AboveMedianPrice")
accuracy = evaluator.evaluate(predictions)

param_grid = ParamGridBuilder() \
    .addGrid(rf.numTrees, [10, 20, 30]) \
    .addGrid(rf.maxDepth, [5, 10, 15]) \
    .addGrid(rf.featureSubsetStrategy, ["auto", "sqrt", "log2"]) \
    .addGrid(rf.minInstancesPerNode, [1, 5, 10]) \
    .build()
evaluator = BinaryClassificationEvaluator(labelCol="AboveMedianPrice")
cross_validator = CrossValidator(estimator=pipeline,
                                 estimatorParamMaps=param_grid,
                                 evaluator=evaluator,
                                 numFolds=5)
cv_model = cross_validator.fit(train_data)
predictions = cv_model.transform(test_data)
accuracy = evaluator.evaluate(predictions)